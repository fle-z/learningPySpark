{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class pyspark.sql.SparkSession(sparkContext, jsparkSession=None)\n",
    "用DataSet和DataFrame编写Spark程序的入口\n",
    "\n",
    "SparkSession的功能包括：\n",
    "+ 创建DataFrame\n",
    "+ 以关系型数据库中表的形式生成DataFrame，之后便可以执行SQL语句，适合小数据量的操作\n",
    "+ 读取.parquet格式的文件，得到DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3.5\"\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .appName(\"TopN\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模拟访问数据记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(product='product7', url='url38'),\n",
       " Row(product='product5', url='url66'),\n",
       " Row(product='product10', url='url64'),\n",
       " Row(product='product9', url='url41'),\n",
       " Row(product='product8', url='url24')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"product\" + str(random.randint(1, 10)) + \" url\" + \\\n",
    "       str(random.randint(1, 100)) for index in range(1000)]\n",
    "table = sc.parallelize(data)\n",
    "table.cache()    # 相当于调用persist(MEMORY_ONLY)， 将RDD作为反序列化对象存储于JVM中 \n",
    "tableRDD = table.map(lambda line: line.split(\" \"))\\\n",
    "            .map(lambda words: Row(product=words[0], url=words[1]))\n",
    "    \n",
    "#注册为临时表，即将RDD转换成DataFrame\n",
    "tableSchema = spark.createDataFrame(tableRDD)\n",
    "tableSchema.createOrReplaceTempView(\"product_url\")\n",
    "result = spark.sql(\"select * from product_url\")\n",
    "result.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计各个产品线下各个URL的访问次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(product='product1', url='url59', access=3),\n",
       " Row(product='product3', url='url90', access=1),\n",
       " Row(product='product8', url='url21', access=1),\n",
       " Row(product='product6', url='url41', access=2),\n",
       " Row(product='product9', url='url92', access=2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accessRDD = spark.sql(\"select product, url ,count(*) as access from product_url group by product, url\")\n",
    "accessRDD.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分区排序取值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark RDD为分区排序提供了非常方便的API：repartitionAndSortWithinPartitions(numPartition, partitionFunc, ascending, keyFunc)  \n",
    "依据partitioner对RDD进行分区，并且在每个结果分区中按key进行排序  \n",
    "这个操作可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('product1', 'url44', 6), ('product1', 'url58', 4), ('product1', 'url81', 3), ('product10', 'url12', 3), ('product10', 'url54', 3), ('product2', 'url35', 6), ('product2', 'url89', 4), ('product2', 'url55', 4), ('product2', 'url84', 3), ('product2', 'url6', 3)]\n"
     ]
    }
   ],
   "source": [
    "accessPairRDD = accessRDD.rdd.map(lambda row: ((row.product, row.url, row.access), None))\n",
    "def partitionFunc(key):\n",
    "    return int(key[0][7]) - 1    # 这里key的数据类型为Row\n",
    "\n",
    "def keyFunc(key):\n",
    "    return key[2]\n",
    "\n",
    "repartitionAndSortRDD = accessPairRDD.repartitionAndSortWithinPartitions(\\\n",
    "                        numPartitions=10, partitionFunc=partitionFunc, ascending=False, keyfunc=keyFunc)\n",
    "#分区排序之后，我们需要将不同分区的前N个值汇总，通过mapPartitions实现\n",
    "#mapPartitions需要一个函数f作为参数，该函数f的参数作为一个“迭代器”，通过这个“迭代器”可以遍历分区内的所有数据\n",
    "N = 5\n",
    "def topNFunc(iter):\n",
    "    buffer = []\n",
    "    for pair in iter:\n",
    "        if len(buffer) >= N:\n",
    "            break\n",
    "            \n",
    "        buffer.append(pair[0])\n",
    "    \n",
    "    return buffer\n",
    "\n",
    "rows = repartitionAndSortRDD.mapPartitions(topNFunc).collect()\n",
    "# 返回值是list\n",
    "print(rows[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们要进行**二次排序**，即：先根据product的序号进行排序，再在此基础上按URL序号排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('product1', 'url44', 6), ('product1', 'url58', 4), ('product1', 'url81', 3), ('product10', 'url12', 3), ('product10', 'url54', 3), ('product2', 'url35', 6), ('product2', 'url55', 4), ('product2', 'url6', 3), ('product2', 'url84', 3), ('product2', 'url89', 4)]\n"
     ]
    }
   ],
   "source": [
    "#对结果进行排序\n",
    "rows_sorted = sorted(rows, key=lambda row: (row[0], row[1]))\n",
    "print(rows_sorted[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort 与 sorted 区别：\n",
    "sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。  \n",
    "list 的 sort 方法返回的是对已经存在的列表进行操作，无返回值，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。  \n",
    "注意：在Python3中不能使用cmp函数"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
