{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, sum, lag, rank\n",
    "import time\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3.5\"\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .appName(\"SAM\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "gravity_data = spark.read.text(\"hdfs://master:9000/dataset/GravityData/1/高台_X212MPET0028_2015_1_9_原始观测数据.tsf\")\n",
    "gravity_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='[TSF-file] v01.0'),\n",
       " Row(value=''),\n",
       " Row(value='[UNDETVAL] 999999.000'),\n",
       " Row(value=''),\n",
       " Row(value='[TIMEFORMAT] DATETIME'),\n",
       " Row(value=''),\n",
       " Row(value='[INCREMENT]   1'),\n",
       " Row(value=''),\n",
       " Row(value='[CHANNELS]'),\n",
       " Row(value='������ϫ�۲�ֵ(2121)'),\n",
       " Row(value='�����ⲿ��ѹ(2128)'),\n",
       " Row(value=''),\n",
       " Row(value='[UNITS]'),\n",
       " Row(value='10-8��m/s2'),\n",
       " Row(value='Pa'),\n",
       " Row(value=''),\n",
       " Row(value=''),\n",
       " Row(value='[DATA]'),\n",
       " Row(value='2015  1   9   0   0   0   13121.61    882.81      '),\n",
       " Row(value='2015  1   9   0   0   1   13124.63    882.7       ')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gravity_data.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行预处理,去除前面的无用行\n",
    "包含的操作有RDD与DataFrame的互换  \n",
    "设置DataFrame的列名,还可以使用...toDF(\"year\", \"month\", ...)  \n",
    "cast()/astype()都能改变DataFrame的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+--------+----+------+-----+------+----+\n",
      "|air_pressure|day| gravity|hour|minute|month|second|year|\n",
      "+------------+---+--------+----+------+-----+------+----+\n",
      "|      882.81|  9|13121.61|   0|     0|    1|     0|2015|\n",
      "|       882.7|  9|13124.63|   0|     0|    1|     1|2015|\n",
      "|      882.81|  9| 13117.5|   0|     0|    1|     2|2015|\n",
      "|      882.81|  9|13129.17|   0|     0|    1|     3|2015|\n",
      "|      882.59|  9|13133.98|   0|     0|    1|     4|2015|\n",
      "+------------+---+--------+----+------+-----+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gravity_fields = gravity_data.rdd.map(lambda row: row.value.split())\n",
    "gravity_fields.take(5)\n",
    "gravity_fields_preprocess = gravity_fields.filter(lambda row: len(row) > 3)\\\n",
    "                            .map(lambda row: Row(year=row[0], month=row[1], day=row[2], hour=row[3], \\\n",
    "                                 minute=row[4], second=row[5], gravity=row[6], air_pressure=row[7]))\\\n",
    "                            .toDF()\n",
    "\n",
    "gravity_fields_preprocess.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移动平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+--------+----+------+-----+------+----+------------------+\n",
      "|air_pressure|day| gravity|hour|minute|month|second|year|         movingAvg|\n",
      "+------------+---+--------+----+------+-----+------+----+------------------+\n",
      "|      883.14|  9| 13146.3|   7|     0|    1|     0|2015|         13156.035|\n",
      "|      883.03|  9|13165.77|   7|     1|    1|     0|2015|13154.013333333334|\n",
      "|      883.03|  9|13149.97|   7|     2|    1|     0|2015|13160.463333333333|\n",
      "|      883.14|  9|13165.65|   7|     3|    1|     0|2015|13160.233333333332|\n",
      "|      883.25|  9|13165.08|   7|     4|    1|     0|2015|13163.230000000001|\n",
      "+------------+---+--------+----+------+-----+------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wSpec = Window.partitionBy(\"hour\")\\\n",
    "                .orderBy(\"second\")\\\n",
    "                .rowsBetween(-1, 1)\n",
    "gravity_fields_preprocess.withColumn(\"movingAvg\", avg(gravity_fields_preprocess[\"gravity\"]).over(wSpec)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 窗口函数主要包括两部分:\n",
    "* 指定窗口特征(wSpec)  \n",
    "partitionBy定义数据如何分组  \n",
    "orderBy定义分组中的排序  \n",
    "rowsBetween定义窗口大小，(-1, 1)表示从前一行到后一行\n",
    "* 指定窗口函数的操作  \n",
    "可以使用ｐｙｓｐａｒｋ.sql.functions的＂聚合函数(Aggregate Function)＂和＂窗口函数(Window Function)＂类别下的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 累计求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+--------+----+------+-----+------+----+--------+\n",
      "|air_pressure|day| gravity|hour|minute|month|second|year|  calSum|\n",
      "+------------+---+--------+----+------+-----+------+----+--------+\n",
      "|      883.14|  9| 13146.3|   7|     0|    1|     0|2015| 13146.3|\n",
      "|      883.03|  9|13165.77|   7|     1|    1|     0|2015|13165.77|\n",
      "|      883.03|  9|13149.97|   7|     2|    1|     0|2015|13149.97|\n",
      "|      883.14|  9|13165.65|   7|     3|    1|     0|2015|13165.65|\n",
      "|      883.25|  9|13165.08|   7|     4|    1|     0|2015|13165.08|\n",
      "|      883.14|  9|13158.96|   7|     5|    1|     0|2015|13158.96|\n",
      "|      883.37|  9|13154.79|   7|     6|    1|     0|2015|13154.79|\n",
      "+------------+---+--------+----+------+-----+------+----+--------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wSpec = Window.partitionBy(\"hour\")\\\n",
    "                .orderBy(\"second\")\\\n",
    "                .rowsBetween(0, 0)\n",
    "gravity_fields_preprocess.withColumn(\"calSum\", sum(gravity_fields_preprocess[\"gravity\"]).over(wSpec)).show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
